---
title: "Lab4"
author: "sbsambado"
date: "12/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load_packages, message= FALSE}

suppressMessages(library(tidyverse)) 
library(effsize)
library(pwr)
library(knitr)
```


```{r}
nc_births <- read_csv("nc_birthweights.csv")


names(nc_births)
class(nc_births)
summary(nc_births)
```

### 2. case_when

Let's update the 'smoke' column so that the words are meaningful, instead of just 0's and 1's, using the case_when() function:

```{r case_when}
# Update 0s and 1s in 'smoke' column to 'nonsmoker' and 'smoker'

# First, make new data frame that has 0s and 1s replaced with 'nonsmoker' and 'smoker,' and adds that to a new column sdp ("smoke during pregnancy"). Then only keep columns tpounds and sdp.

nc_new <- nc_births %>% 
  mutate(
    sdp = case_when(
      smoke == 0 ~ "nonsmoker",
      smoke == 1 ~ "smoker")
  ) %>% 
  filter(sdp != "NA") %>% 
  select(sdp, gained, weeks, tpounds)
  

```


### 3. Go exploring

- Histograms
- QQ plots
- Data structure (names, class, etc.)
- Formal test for normality (shapiro.test)

A bubble plot: size of points based on a variable value

Note: change the height and width of R-generated figures when knitted using fig.height and fig.width, and alignment using fig.align = "center", in the code chunk header

```{r data_exploration, fig.wdith = 5, fig.height = 4, fig.align="center}

bw_scatter <- ggplot(nc_new, aes(x = weeks, y = tpounds)) +
  geom_point(aes(color = sdp, size = gained), 
             alpha = 0.3) +
  theme_classic() +
  scale_color_manual(values = c("purple","orange"), name = "Smoker\nStatus") +
  labs(x = "Gestation time (weeks)", y = "Birth weight (pounds)") +
  scale_size_continuous(name = "Mother\nWeight Gain")
bw_scatter


```

```{r hist_qq}

birth_hist <- ggplot(nc_new, aes(x = tpounds)) +
  geom_histogram(aes(fill  = sdp)) +
  facet_wrap(~ sdp, scale = "free")
birth_hist # pretty normal dist., but large sample size

counts <- nc_new %>% 
  count(sdp) # Notice that 'count' does group_by + length for you

birth_qq <- ggplot(nc_new, aes(sample = tpounds)) +
  geom_qq(aes(fill = sdp)) +
  facet_wrap(~ sdp, scale = "free")
birth_qq

```
Summary information table

```{r}
birth_summary <- nc_new %>% 
  group_by(sdp) %>% 
  summarize(
    mean_wt = mean(tpounds),
    sd_wt = sd(tpounds),
    max_wt = max(tpounds),
    min_wt = min(tpounds),
    sample_size = length(tpounds)
  )

kable(birth_summary)
  
```



###4. A formal hypothesis test for normality

Be cautious of these tests - they can lead to bad binary decisions. If you have a large sample size, you will almost always reject the null hypothesis of normality (even if very close...). If you have small sample size, you will almost NEVER reject the null hypothesis of normality (even if it looks very non-normal). 




```{r}
# First, make VECTORS containing just the weights for babies born to smoking or non-smoking mothers:

s <- nc_new %>% 
  filter(sdp == "smoker") %>% 
  pull(tpounds)

ns <- nc_new %>% 
  filter(sdp == "nonsmoker") %>% 
  pull(tpounds)

# Now we have two vectors containing only birth weight values for smoking (s) and nonsmoking (ns) mothers


# A formal test for normality: Shapiro Wilk

shapiro.test(s) # W = 0.94413, p-value = 5.421e-05
shapiro.test(ns) # W = 0.92603, p-value < 2.2e-16


```

### 5. F-Test for equal variances

```{r}
#H0: The ratio of sample variances = 1 (variances are equal)
#H1: The ratio of sample variances is NOT 1 (variances are unequal)

f_test <- var.test(s,ns)
f_test  # p = 0.1973 (p > 0.05)

# Conclude: variances are equal (also, general rule: if largest sample variance is < 4x greater than the smallest sample variance, then usually tests are OK)

```

### 5. Two sample t-tests to compare means

**Question 1: Is there a significant difference in birthweight for babies born to smoking versus non-smoking mothers?**

```{r}

# H0: There is not a significant difference in birthweights for babies born to smoking/non-smoking mothers
# HA: There IS a significant difference in mean birthweights

# alpha = 0.05 (two-tailed)
t_diff <- t.test(s, ns)
t_diff # p = 0.01937 (p < 0.05)

# Conclusion: There is a significant difference in mean birthweight for babies born to smoking and non-smoking  mothers


```


**Question 2: Do babies born to smoking mothers have a lower mean birthweight than those born to non-smoking mothers?**

```{r}
#H0: Birthweight for babies born to smoking mothers is NOT lower than for non-smoking mothers
#HA: Birthweight for babies born to smoking mothers IS lower than for non-smoking mothers

t_s_less_ns <- t.test(s, ns, alternative = "less")
t_s_less_ns # p = 0.009684 (p < 0.05)

# Conclusion: Babies born to smoking mothers are significantly smaller (birth weight) than those born to non-smoking mothers (t(171) = -2.4, p = 0.01). 


```

Reminder: in the examples above, we are comparing means for UNPAIRED samples. What if we have samples that *are* paired (each observation in one sample is associated with one observation in the other sample)? Then we'd want to use a paired t-test. 

Just do that by adding argument *paired = TRUE* in the t.test() function.


###6. Beyond the p-value: effect size (package 'effsize')

Remember that the p-value only tells part of the story. We choose a cut-off point where we'll either reject or retain the null hypothesis. But if we have a large enough sample size, then you can find a significant difference between means no matter how close together they are. 

We should start thinking about different ways to discuss the **magnitude of differences** between samples in addition to reporting p-values. One way is *effect size*, which we'll calculate using Cohen's d. 

Example: Find the effect size (Cohen's d) for the babies of smoking/non-smoking mothers. 

```{r cohen_d}

# Calculate Cohen's d for effect size btwn smoking/non-smoking
effect_size <- cohen.d(ns,s)


effect_size # d-estimate: 0.21 (small effect size)

 # This is a small size

# Conclusion: While babies born to smoking mothers are significantly smaller in weight than those born to non-smoking mothers (t(171) = -2.4, p = 0.01), the effect size is small (Cohen's *d* = 0.21), with only a 0.31 pound difference between mean weights. Further, mean weights for babies born to both smoking and non-smoking mothers are well above low birthweight criteria. << Always think about CONTEXT!

```

### 7. Power Analysis (package 'pwr')

The type of power analysis that you'll do depends on the TEST. Which means that there's a different power calculation for each type of hypothesis test you'll run. 

For example, to find values associated with power for a t-test, you'll use pwr.t.test(), but if you're doing a calculation for an ANOVA, you'll use pwr.anova.test().

Remember, the POWER of a test is the probability that you will detect a significant result if there really is one. It is the complement of committing a Type II Error, $\beta$ (there IS a significant result but you don't detect it). 

First, check out the pwr.t.test() function (make sure 'pwr' package is loaded - if at home, they'll need to install first)

**Example:** You need to collect samples to test a hypothesis that lagoons downstream from golf courses contain higher phosphate concentrations that those not downstream from golf courses. If:

A. you plan to use a two-sample t-test to compare phosphate concentrations, 
B. your significance level is 0.05,
C. you want to have a power of 0.80

...then how many samples would you have to collect if there is a SMALL (d ~ 0.2), MODERATE (d ~ 0.5), or LARGE (d ~ 0.8) effect size? First of all, how can we GUESS what the effect size might be if we have *a priori* information? It's *basically* the difference in means divided by the pooled standard deviation. So you could estimate what the effect size will be, or just try endpoints for low and high effect size.

To use pwr.t.test() function, there are four components:

n = sample size
d = Cohen's d effect size
sig.level = alpha
power = power (standard is ~ 0.8)

If you give the function THREE of those things, and set the fourth to NULL, then the fourth thing will be calculated for you.


```{r a_priori_power_calcs}

# small effect size

power_small <- pwr.t.test(n = NULL, d = 0.2, sig.level = .05, power = 0.8)
power_small # ~393

power_medium <- pwr.t.test(n = NULL, d = 0.5, sig.level = .05, power = 0.8)
power_medium # ~ 64

power_large <- pwr.t.test(n = NULL, d = 0.8, sig.level = .05, power = 0.8)
power_large # ~ 25

```

What if we want to calculate the power associated with a test we've already done? This isn't usually super interesting, but you can do it. 

**Example:** You've already performed a t-test using two samples for lagoons downstream from golf courses and those not downstream from golfcourses (n = 40 for each), finding an effect size (Cohen's d) of 0.6. What is the power associated with your test if your significance level is 0.05?

```{r post_hoc_power}
power_post_hoc <- pwr.t.test(n = 40, d = 0.6, sig.level = 0.05, p = NULL)
power_post_hoc
```

You had a 75% chance of finding a significant difference if there really is one. Does this matter post-hoc? Not really...you either decided you found a significant differene or you didn't. Post-hoc power kind of becomes just an academic question that isn't super useful it making decisions about data. 
